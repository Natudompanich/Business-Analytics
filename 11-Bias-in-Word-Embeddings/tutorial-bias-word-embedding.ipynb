{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "### Powerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems üîéü§ñüß∞\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "### Legend:\n",
    "# üíé Important\n",
    "# ‚ö° Be Aware - Debated issue / interpret carefully / simplicity over precision\n",
    "# üõ†Ô∏è Setup/Technical (a.k.a \"the code is not important, just run it!\")\n",
    "# üß™ Methodological Issue\n",
    "# üíª Hands-On - Your turn! NO programming background\n",
    "# ‚å®Ô∏è ... Some programming background (in Python) is required\n",
    "# ü¶Ñ Out of Scope\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "# Part One: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - üõ†Ô∏è Install `responsibly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user git+https://github.com/ResponsiblyAI/responsibly.git@dev\n",
    "# !pip install --user responsibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - üõ†Ô∏è Validate Installation of `responsibly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import responsibly\n",
    "\n",
    "# You should get '0.1.2'\n",
    "responsibly.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ö†Ô∏è\n",
    "\n",
    "\n",
    "If you get an error of **`ModuleNotFoundError: No module named 'responsibly'`** after the installation, and you work on either **Colab** or **Binder** - this is **normal**.\n",
    "<br/> <br/>\n",
    "**Restart** the Kernel/Runtime (use the menu on top or the botton in the notebook), **skip** the installation cell (`!pip install --user responsibly`) and **run** the previous cell again (`import responsibly`).\n",
    "\n",
    "###  Now it should all work fine!\n",
    "\n",
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Examples of Bias in Language Technology\n",
    "\n",
    "## 2.1 - Translation\n",
    "\n",
    "![](images/example-translate.jpg)\n",
    "\n",
    "Source: [Google Blog](https://www.blog.google/products/translate/reducing-gender-bias-google-translate/), [Google AI Blog](https://ai.googleblog.com/2018/12/providing-gender-specific-translations.html)\n",
    "## 2.2 - Automated Speech Recognition (ASR) \n",
    "\n",
    "![](images/asr-wer.jpg)\n",
    "\n",
    "WER = Average Word Error Rate\n",
    "\n",
    "`(substitutions + deletions + insertions) / total number of words`\n",
    "\n",
    "Koenecke, Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. \"[Racial disparities in automated speech recognition](https://www.pnas.org/content/117/14/7684).\" Proceedings of the National Academy of Sciences 117, no. 14 (2020): 7684-7689.\n",
    "\n",
    "[Stanford News](https://news.stanford.edu/2020/03/23/automated-speech-recognition-less-accurate-blacks/)\n",
    "\n",
    "## 2.3 - Recruiting tool\n",
    "\n",
    "### \"Amazon scraps secret AI recruiting tool that showed bias against women\" ([Reuters](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G))\n",
    "\n",
    "\"But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.\"\n",
    "\n",
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Motivation - Why to use Word Embeddings?\n",
    "\n",
    "## 3.1 - [NLP (Natural Language Processing)](https://en.wikipedia.org/wiki/Natural_language_processing) - **Very partial** list of tasks\n",
    "\n",
    "\n",
    "### 1. Classification\n",
    "- Fake news classification\n",
    "- Toxic comment classification\n",
    "- Review raiting (sentiment analysis)\n",
    "- Hiring decision making by CV\n",
    "- Automated essay scoring\n",
    "\n",
    "### 3. Machine Translation\n",
    "\n",
    "### 2. Information Retrieval\n",
    "- Search engine\n",
    "- Plagiarism detection\n",
    "\n",
    "### 3. Conversation chatbot\n",
    "\n",
    "### 4. Coreference Resolution\n",
    "![](images/corefexample.png)\n",
    "<small>Source: [Stanford Natural Language Processing Group](https://nlp.stanford.edu/projects/coref.shtml)</small>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## 3.3 - Machine Learning (NLP) Pipeline\n",
    "<br>\n",
    "<div style=\"border: 1px solid; padding: 50px; margin: 10px\">\n",
    " <h2>\n",
    " \n",
    "Data ‚Üí Representation ‚Üí (Structured) Inference ‚Üí Prediction   \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üë\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Auxiliary Corpus/Model\n",
    " </h2>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<small>Source: [Kai-Wei Chang (UCLA) - What It Takes to Control Societal Bias in Natural Language Processing](https://www.youtube.com/watch?v=RgcXD_1Cu18)</small>\n",
    "\n",
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2> 3.3 - Esessional Question - How to represent language to machine?</h2>\n",
    "</center>\n",
    "\n",
    "## We need some kind of *dictionary* üìñ to transform/encode\n",
    "## ... from a human representation (words) üó£ üî°\n",
    "## ... to a machine representation (numbers) ü§ñ üî¢\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## First Try\n",
    "\n",
    "### Idea: Bag of Words (for a document)\n",
    "![](images/bow.png)\n",
    "<small>Source: Zheng, A.& Casari, A. (2018). Feature Engineering for Machine Learning. O'Reilly Media.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocabulary = ['it', 'they', 'puppy', 'and', 'cat', 'aardvark', 'cute', 'extreamly', 'not']\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'it is puppy and it extreamly cute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform([sentence]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(['it is not puppy and it extreamly cute']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(['it is puppy and it extreamly not cute']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vectorizer.fit_transform([word]).toarray()\n",
    " for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Representation - The Issue with Text\n",
    "\n",
    "![](images/audio-image-text.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>\n",
    "\n",
    "[Color Picker](https://www.google.com/search?q=color+picker)\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## 3.4 - üíé Idea: Embedding a word in a n-dimensional space\n",
    "\n",
    "### Distributional Hypothesis\n",
    "> \"a word is characterized by the company it keeps\" - [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth)\n",
    "\n",
    "#### ü¶Ñ Training: using *word-context* relationships from a corpus. See: [The Illustrated Word2vec by Jay Alammar](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "### Distance ~ Meaning Similarity\n",
    "\n",
    "### ü¶Ñ Examples (algorithms and pre-trained models)\n",
    "- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText](https://fasttext.cc/)\n",
    "- [ELMo](https://allennlp.org/elmo) (contextualized)\n",
    "\n",
    "#### ü¶Ñ State of the Art\n",
    "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\n",
    "](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four: Let's play with Word2Vec word embedding...!\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) - Google News - 100B tokens, 3M vocab, cased, 300d vectors - only lowercase vocab extracted\n",
    "\n",
    "Loaded using [`responsibly`](http://docs.responsibly.ai) package, the function [`responsibly.we.load_w2v_small`]() returns a [`gensim`](https://radimrehurek.com/gensim/)'s [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) object.\n",
    "\n",
    "\n",
    "## 4.1 - Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è‚ö° ignore warnings\n",
    "# generally, you shouldn't do that, but for this tutorial we'll do so for the sake of simplicity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import load_w2v_small\n",
    "\n",
    "w2v_small = load_w2v_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "\n",
    "len(w2v_small.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the word \"home\"\n",
    "\n",
    "print('home =', w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the word embedding dimension, in this case, is 300\n",
    "\n",
    "len(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the words are normalized (=have norm equal to one as vectors)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "norm(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è make sure that all the vectors are normalized!\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "length_vectors = norm(w2v_small.vectors, axis=1)\n",
    "\n",
    "assert_almost_equal(actual=length_vectors,\n",
    "                    desired=1,\n",
    "                    decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - üíé Demo - Mesuring Distance between Words\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Sphere_wireframe_10deg_6r.svg/480px-Sphere_wireframe_10deg_6r.svg.png)\n",
    "\n",
    "### Mesure of Similiarty: [Cosine Similariy](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "#### Measures the cosine of the angle between two vecotrs.\n",
    "#### Ranges between 1 (same vector) to -1 (opposite/antipode vector)\n",
    "\n",
    "#### In Python, for normalized vectors (Numpy's array), use the `@`(at) operator!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, degrees\n",
    "\n",
    "degrees(acos(w2v_small['cat'] @ w2v_small['cats']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['cow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['graduated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['graduated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíé In general, the use of Word Embedding to encode words, as an input for NLP systems (*), improve their performance compared to one-hot representation.\n",
    "\n",
    "* Sometimes the embedding is learned as part of the NLP system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - üõ†Ô∏è Demo - Visualization Word Embedding in 2D using T-SNE \n",
    "\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "# take the most common words in the corpus between 200 and 600\n",
    "words = [word for word in w2v_small.index2word[200:600]]\n",
    "\n",
    "# convert the words to vectors\n",
    "embeddings = [w2v_small[word] for word in words]\n",
    "\n",
    "# perform T-SNE\n",
    "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "# ... and visualize!\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, label in enumerate(words):\n",
    "    x, y = words_embedded[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                 ha='right', va='bottom', size=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Demo - [Tensorflow Embedding Projector](http://projector.tensorflow.org)\n",
    "\n",
    "‚ö° Be cautious: It is easy to see \"patterns\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Demo - Most Similar\n",
    "\n",
    "What are the most simlar words (=closer) to a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 - [EXTRA] Demo - Doesn't Match\n",
    "\n",
    "Given a list of words, which one doesn't match?\n",
    "\n",
    "The word further away from the mean of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 - Demo - Vector Arithmetic\n",
    "\n",
    "![](images/vector-addition.png)\n",
    "\n",
    "<small>Source: [Wikipedia](https://commons.wikimedia.org/wiki/File:Vector_add_scale.svg)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature + science = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['nature', 'science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 - üíé More Vector Arithmetic\n",
    "\n",
    "![](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 - Demo - Vector Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man:king :: woman:?\n",
    "# king - man + woman = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['king', 'woman'],\n",
    "                       negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['big', 'smaller'],\n",
    "                       negative=['small'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 - Think about a DIRECTION in word embedding as a RELATION\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$\n",
    "# $\\overrightarrow{smaller} - \\overrightarrow{small}$\n",
    "# $\\overrightarrow{Spain} - \\overrightarrow{Madrid}$\n",
    "\n",
    "\n",
    "### ‚ö° Direction is not a word vector by itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° But it doesn't work all the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['forward', 'up'],\n",
    "                       negative=['down'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be because we have the\"looking forward\" which is acossiated with \"excitement\" in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°ü¶Ñ Keep in mind the word embedding was generated by learning the co-occurrence of words, so the fact that it *empirically* exhibit \"concept arithmetic\", it doesn't necessarily mean it learned it! In fact, it seems it didn't.\n",
    "See: [king - man + woman is queen; but why? by Piotr Migda≈Ç](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
    "\n",
    "### ü¶Ñ [Demo - Word Analogies Visualizer by Julia Bazi≈Ñska](https://lamyiowce.github.io/word2viz/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°ü¶Ñ In fact, `w2v_small.most_similar` find the most closest word which *is not one* of the given ones. This is a real methodological issue. Nowadays, it is not a common practice to evaluate word embedding with analogies.\n",
    "\n",
    "You can use `from responsibly.we import most_similar` for the unrestricted version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Part Five: Gender Bias\n",
    "\n",
    "### ‚ö° We use the word *bias* merely as a technical term, without jugement of \"good\" or \"bad\". Later on we will put the bias into *human contextes* to evaluate it.\n",
    "\n",
    "Keep in mind, the data is from Google News, the writers are professional journalists.\n",
    "\n",
    "### Bolukbasi Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). NIPS 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Gender appropriate he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# she:sister :: he:?\n",
    "# sister - she + he = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['sister', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "queen-king\n",
    "waitress-waiter\n",
    "sister-brother\n",
    "mother-father\n",
    "ovarian_cancer-prostate_cancer\n",
    "convent-monastery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Gender stereotype he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['nurse', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sewing-carpentry\n",
    "nurse-doctor\n",
    "blond-burly\n",
    "giggle-chuckle\n",
    "sassy-snappy\n",
    "volleyball-football\n",
    "register_nurse-physician\n",
    "interior_designer-architect\n",
    "feminism-conservatism\n",
    "vocalist-guitarist\n",
    "diva-superstar\n",
    "cupcakes-pizzas\n",
    "housewife-shopkeeper\n",
    "softball-baseball\n",
    "cosmetics-pharmaceuticals\n",
    "petite-lanky\n",
    "charming-affable\n",
    "hairdresser-barber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But with the unrestricted version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(w2v_small,\n",
    "             positive=['nurse', 'he'],\n",
    "             negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Be Aware: According to a recent paper, it seems that the method of generating analogies enforce producing gender sterotype ones!\n",
    "\n",
    "Nissim, M., van Noord, R., van der Goot, R. (2019). [Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor](https://arxiv.org/abs/1905.09866).\n",
    "\n",
    "... and a [Twitter thread](https://twitter.com/adamfungi/status/1133865428663635968) between the authors of the two papares.\n",
    "\n",
    "## My takeaway (and as well as of other researchers): Analogies are not approriate method to observe bias in word embedding.\n",
    "\n",
    "## üß™ What if our methodology introduce a bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - üíé What can we take from analogies? Gender Direction!\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction = w2v_small['she'] - w2v_small['he']\n",
    "\n",
    "gender_direction /= norm(gender_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['architect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['interior_designer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°Interprete carefully: The word *architect* appears in more contexts with *he* than with *she*, and vice versa for *interior designer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶Ñ In practice, we calculate the gender direction using multiple definitional pair of words for better estimation (words may have more than one meaning):\n",
    "\n",
    "- woman - man\n",
    "- girl - boy\n",
    "- she - he\n",
    "- mother - father\n",
    "- daughter - son\n",
    "- gal - guy\n",
    "- female - male\n",
    "- her - his\n",
    "- herself - himself\n",
    "- Mary - John"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 - üíª Try some words by yourself\n",
    "‚ö° Keep in mind: You are performing exploratory data analysis, and not evaluate systematically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 - üíé So What?\n",
    "\n",
    "### Downstream Application - Putting a system into a human context\n",
    "\n",
    "### Toy Example - Search Engine Ranking\n",
    "\n",
    "- \"MIT computer science PhD student\"\n",
    "- \"doctoral candidate\" ~ \"PhD student\"\n",
    "- John:computer programmer :: Mary:homemaker\n",
    "\n",
    "### Universal Embeddings\n",
    "- Pre-trained on a large corpus\n",
    "- Plugged in downstream task models (sentimental analysis, classification, translation ‚Ä¶)\n",
    "- Improvement of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 - Measuring Bias in Word Embedding\n",
    "\n",
    "# Think-Pair-Shar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Basic Ideas: Use neutral-gender words!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# Neutral Professions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 - Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import GenderBiasWE\n",
    "\n",
    "w2v_small_gender_bias = GenderBiasWE(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.positive_end, w2v_small_gender_bias.negative_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender direction\n",
    "w2v_small_gender_bias.direction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we.data import BOLUKBASI_DATA\n",
    "\n",
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Why `actor` is in the neutral profession names list while `actress` is not there?\n",
    "1. Due to the statistical nature of the method that is used to find the gender- specific and natural word\n",
    "2. That might be because `actor` nowadays is much more gender-neutral, compared to waiter-waitress (see [Wikipedia - The term Actress](https://en.wikipedia.org/wiki/Actor#The_term_actress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neutral_profession_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same of using the @ operator on the bias direction\n",
    "\n",
    "w2v_small_gender_bias.project_on_direction(neutral_profession_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the projections of professions (neutral and specific by the orthography) on the gender direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_bias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo - Visualizing gender bias with [Word Clouds](http://wordbias.umiacs.umd.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 - Are the projections of occupation words on the gender direction related to the real world?\n",
    "\n",
    "Let's take the percentage of female in various occupations from the Labor Force Statistics of 2017 Population Survey.\n",
    "\n",
    "Taken from: https://arxiv.org/abs/1804.06876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter  # üõ†Ô∏è For idiomatic sorting in Python\n",
    "\n",
    "from responsibly.we.data import OCCUPATION_FEMALE_PRECENTAGE\n",
    "\n",
    "sorted(OCCUPATION_FEMALE_PRECENTAGE.items(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_bias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also: Word embeddings quantify 100 years of gender stereotypes\n",
    "\n",
    "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). [Word embeddings quantify 100 years of gender and ethnic stereotypes](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf). Proceedings of the National Academy of Sciences, 115(16), E3635-E3644.\n",
    "\n",
    "![](images/gender-bias-over-decades.png)\n",
    "\n",
    "<small>Data: Google Books/Corpus of Historical American English (COHA)</small>\n",
    "\n",
    "Word embedding is sometimes used to analyze a collection of text in **digital humanities** - putting a system into a human context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Quite strong and interesting observation! We used \"external\" data which wan't used directly to create the word embedding.\n",
    "#### It takes us to think about the *data generation process* - in both cases it is the \"world\", but it will be difficult to argue for causality:\n",
    "##### 1. Text in newspapers\n",
    "##### 2. Employment by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 - Direct Bias Measure\n",
    "\n",
    "1. Project each **neutral profession names** on the gender direction\n",
    "2. Calculate the absolute value of each projection\n",
    "3. Average it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using responsibly\n",
    "\n",
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what responsibly does:\n",
    "\n",
    "neutral_profession_projections = [w2v_small[word] @ w2v_small_gender_bias.direction\n",
    "                                  for word in neutral_profession_names]\n",
    "\n",
    "abs_neutral_profession_projections = [abs(proj) for proj in neutral_profession_projections]\n",
    "\n",
    "sum(abs_neutral_profession_projections) / len(abs_neutral_profession_projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ What are the assumptions of the direct bias measure? How the choice of neutral word effect on the definition of the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 - [EXTRA] Indirect Bias Measure\n",
    "Similarity due to shared \"gender direction\" projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.generate_closest_words_indirect_bias('softball',\n",
    "                                                           'football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Six: Mitigating Bias\n",
    "\n",
    "> We intentionally do not reference the resulting embeddings as \"debiased\" or free from all gender bias, and\n",
    "prefer the term \"mitigating bias\" rather that \"debiasing,\" to guard against the misconception that the resulting\n",
    "embeddings are entirely \"safe\" and need not be critically evaluated for bias in downstream tasks. - <small>James-Sorenson, H., & Alvarez-Melis, D. (2019). [Probabilistic Bias Mitigation in Word Embeddings](https://arxiv.org/pdf/1910.14497.pdf). arXiv preprint arXiv:1910.14497.</small>\n",
    "\n",
    "\n",
    "## 6.1 - Neutralize\n",
    "\n",
    "In this case, we will remove the gender projection from all the words, except the neutral-gender ones, and then normalize.\n",
    "\n",
    "ü¶Ñ We need to \"learn\" what are the gender-specific words in the vocabulary for a seed set of gender-specific words (by semi-automatic use of [WordNet](https://en.wikipedia.org/wiki/WordNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='neutralize', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_debias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 [EXTRA] Equalize\n",
    "\n",
    "- Do you see that `man` and `woman` have a different projection on the gender direction? \n",
    "\n",
    "- It might cause to different similarity (distance) to neutral words, such as to `kitchen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['equalize_pairs'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Hard Debias = Neutralize + Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='hard', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 - Compare Preformances\n",
    "\n",
    "After debiasing, the performance of the word embedding, using standard benchmarks, get only slightly worse!\n",
    "\n",
    "### ‚ö†Ô∏è It might take few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé Part Seven: So What?\n",
    "\n",
    "We removed the gender bias, **as we defined it**, in a word embedding - Is there any impact on a downstream application?\n",
    "\n",
    "\n",
    "### Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2018). [Gender bias in coreference resolution: Evaluation and debiasing methods](https://par.nsf.gov/servlets/purl/10084252). NAACL-HLT 2018.\n",
    "\n",
    "\n",
    "#### WinoBias Dataset\n",
    "![](images/coref-example.png)\n",
    "\n",
    "\n",
    "#### Stereotypical Occupations (the source of `responsibly.we.data.OCCUPATION_FEMALE_PRECENTAGE`)\n",
    "![](images/coref-occupations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results on *UW End-to-end Neural Coreference Resolution System*\n",
    "\n",
    "##### No Intervention - Baseline\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   67.7   |            76.0            |             49.4            | 62.7 | 26.6* |\n",
    "\n",
    "##### Intervention: Named-entity anonymization\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   66.4   |            73.5            |             51.2            | 62.6 | 21.3* |\n",
    "|  Hard Debiased |   66.5   |            67.2            |             59.3            | 63.2 |  7.9* |\n",
    "\n",
    "##### Interventions: Named-entity anonymization + Gender swapping\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   66.2   |            65.1            |             59.2            | 62.2 |  5.9* |\n",
    "|  Hard Debiased |   66.3   |            63.9            |             62.8            | 63.4 |  1.1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhao, J., Zhou, Y., Li, Z., Wang, W., & Chang, K. W. (2018). [Learning gender-neutral word embeddings](https://arxiv.org/pdf/1809.01496.pdf). EMNLP 2018.\n",
    "\n",
    "#### Another bias mitigation method (tailor-made for GloVe training process)\n",
    "\n",
    "![](images/gn-glove-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíéüíé Part Eight: Meta \"So What?\" - I\n",
    "\n",
    "## How should we definition of \"bias\" in word embedding?\n",
    "\n",
    "### 1. Intrinsic (e.g., direct bias)\n",
    "\n",
    "### 2. External - Downstream application (e.g., coreference resolution, classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé Part Nine: Have we really removed the bias?\n",
    "\n",
    "Let's look on another metric, called **WEAT** (Word Embedding Association Test) which is inspired by **IAT** (Implicit-Association Test) from Pyschology.\n",
    "\n",
    "### Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). [Semantics derived automatically from language corpora contain human-like biases.](http://www.cs.bath.ac.uk/~jjb/ftp/CaliskanEtAl-authors-full.pdf) Science, 356(6334), 183-186.\n",
    "\n",
    "\n",
    "## 9.1 - Ingredients\n",
    "\n",
    "1. Target words (e.g., Male ve. Female)\n",
    "\n",
    "2. Attribute words (e.g., Math vs. Arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy  # üõ†Ô∏è For copying a nested data structure in Python\n",
    "\n",
    "from responsibly.we.weat import WEAT_DATA\n",
    "\n",
    "\n",
    "# B. A. Nosek, M. R. Banaji, A. G. Greenwald, Math=male, me=female, therefore math‚â†me.,\n",
    "# Journal of Personality and Social Psychology 83, 44 (2002).\n",
    "weat_gender_science_arts = deepcopy(WEAT_DATA[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è filter words from the original IAT experiment that are not presend in the reduced Word2Vec model\n",
    "\n",
    "from responsibly.we.weat import _filter_by_model_weat_stimuli\n",
    "\n",
    "_filter_by_model_weat_stimuli(weat_gender_science_arts, w2v_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 - Recipe\n",
    "\n",
    "‚ûï Male x Science\n",
    "\n",
    "‚ûñ Male x Arts\n",
    "\n",
    "‚ûñ Female x Science\n",
    "\n",
    "‚ûï Female x Arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_combination_similiarity(model, attribute, target):\n",
    "    score = 0\n",
    "\n",
    "    for attribute_word in attribute['words']:\n",
    "\n",
    "        for target_word in target['words']:\n",
    "\n",
    "            score += w2v_small.similarity(attribute_word,\n",
    "                                          target_word)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                  weat_gender_science_arts['first_attribute'],\n",
    "                                                  weat_gender_science_arts['first_target'])\n",
    "\n",
    "male_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                               weat_gender_science_arts['first_attribute'],\n",
    "                                               weat_gender_science_arts['second_target'])\n",
    "\n",
    "male_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                    weat_gender_science_arts['second_attribute'],\n",
    "                                                    weat_gender_science_arts['first_target'])\n",
    "\n",
    "female_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                                 weat_gender_science_arts['second_attribute'],\n",
    "                                                 weat_gender_science_arts['second_target'])\n",
    "\n",
    "female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score - male_arts_score - female_science_score + female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weat_gender_science_arts['first_attribute']['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(male_science_score - male_arts_score - female_science_score + female_arts_score) / 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 - All WEAT Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import calc_all_weat\n",
    "\n",
    "calc_all_weat(w2v_small, [weat_gender_science_arts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Important Note: Our results are a bit different because we use a reduced Word2Vec.\n",
    "\n",
    "\n",
    "### Results from the Paper (computed on the complete Word2Vec):\n",
    "\n",
    "![](images/weat-w2v.png)\n",
    "\n",
    "\n",
    "### ‚ö°Caveats regarding comparing WEAT to the IAT\n",
    "\n",
    "- Individuals (IAT) vs. Words (WEAT)\n",
    "- Therefore, the meanings of the effect size and p-value are totally different!\n",
    "\n",
    "### ‚ö°ü¶Ñ The definition of the WEAT score is structured differently (but it is computationally equivalent). The original formulation matters to compute the p-value. Refer to the paper for details.\n",
    "\n",
    "### üß™ With the effect size, we can \"compare\" a human bias to a machine one. It raises the question whether the baseline for meauring bias/fairness of a machine should be human bias? Then a well-performing machine shouldn't be necessarily not biased, but only less biased than human (think about autonomous cars or semi-structured vs. unstructured interview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 - Let's go back to our question - did we removed the bias?\n",
    "\n",
    "\n",
    "### Gonen, H., & Goldberg, Y. (2019, June). [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/pdf/1903.03862.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 609-614).\n",
    "\n",
    "They used multiple methods, we'll show only two:\n",
    "1. WEAT\n",
    "2. Neutral words clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1 -  WEAT - before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/weat-experiment.png)\n",
    "\n",
    "See `responsibly` [demo page on word embedding](https://docs.responsibly.ail/notebooks/demo-word-embedding-bias.html#first-experiment-weat-before-and-after-debias) for a complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 - Clustering Neutral Gender Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = {word for word in w2v_small_gender_bias.model.vocab.keys()}\n",
    "\n",
    "# ü¶Ñ how we got these words - read the Bolukbasi's paper for details\n",
    "all_gender_specific_words = set(BOLUKBASI_DATA['gender']['specific_full_with_definitional_equalize'])\n",
    "\n",
    "all_gender_neutral_words = w2v_vocab - all_gender_specific_words\n",
    "\n",
    "print('#vocab =', len(w2v_vocab),\n",
    "      '#specific =', len(all_gender_specific_words),\n",
    "      '#neutral =', len(all_gender_neutral_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections = [(w2v_small_gender_bias.project_on_direction(word), word)\n",
    "                                    for word in all_gender_neutral_words]\n",
    "\n",
    "neutral_words_gender_projections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neutral words: top 500 male-biased and top 500 female-biased words\n",
    "\n",
    "GenderBiasWE.plot_most_biased_clustering(w2v_small_gender_bias, w2v_small_gender_debias);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the paper, they got a stronger result, 92.5% accuracy for the debiased model.\n",
    "However, they perform clustering on all the words from the reduced word embedding, both gender- neutral and specific words, and applied slightly different pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 - üíé Strong words form the paper (emphasis mine):\n",
    "\n",
    "> The experiments ...\n",
    "reveal a **systematic bias** found in the embeddings,\n",
    "which is **independent of the gender direction**.\n",
    "\n",
    "\n",
    "> The implications are alarming: while suggested\n",
    "debiasing methods work well at removing the gender direction, the **debiasing is mostly superficial**.\n",
    "The bias stemming from world stereotypes and\n",
    "learned from the corpus is **ingrained much more\n",
    "deeply** in the embeddings space.\n",
    "\n",
    "\n",
    "> .. real concern from biased representations is **not the association** of a concept with\n",
    "words such as ‚Äúhe‚Äù, ‚Äúshe‚Äù, ‚Äúboy‚Äù, ‚Äúgirl‚Äù **nor** being\n",
    "able to perform **gender-stereotypical word analogies**... algorithmic discrimination is more likely to happen by associating one **implicitly gendered** term with\n",
    "other implicitly gendered terms, or picking up on\n",
    "**gender-specific regularities** in the corpus by learning to condition on gender-biased words, and generalizing to other gender-biased words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíéüíé Part Ten: Meta \"So What?\" - II\n",
    "\n",
    "## Can we debias at all a word embedding?\n",
    "\n",
    "## Under some downstream use-cases, maybe the bias in the word embedding is desirable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# ‚å®Ô∏è Part Eleven: Your Turn!\n",
    "\n",
    "## Explore bias in word embedding by other groups (such as race and religious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Let's explor racial bias usint Tolga's approche. Will use the [`responsibly.we.BiasWordEmbedding`](http://docs.responsibly.ai/word-embedding-bias.html#ethically.we.bias.BiasWordEmbedding) class. `GenderBiasWE` is a sub-class of `BiasWordEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import BiasWordEmbedding\n",
    "\n",
    "w2v_small_racial_bias = BiasWordEmbedding(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíéüíéüíé Identify the racial direction using the `sum` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_common_names = ['Emily', 'Anne', 'Jill', 'Allison', 'Laurie', 'Sarah', 'Meredith', 'Carrie',\n",
    "                      'Kristen', 'Todd', 'Neil', 'Geoffrey', 'Brett', 'Brendan', 'Greg', 'Matthew',\n",
    "                      'Jay', 'Brad']\n",
    "\n",
    "black_common_names = ['Aisha', 'Keisha', 'Tamika', 'Lakisha', 'Tanisha', 'Latoya', 'Kenya', 'Latonya',\n",
    "                      'Ebony', 'Rasheed', 'Tremayne', 'Kareem', 'Darnell', 'Tyrone', 'Hakim', 'Jamal',\n",
    "                      'Leroy', 'Jermaine']\n",
    "\n",
    "w2v_small_racial_bias._identify_direction('Whites', 'Blacks',\n",
    "                                          definitional=(white_common_names, black_common_names),\n",
    "                                          method='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the neutral profession names to measure the racial bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_racial_bias.plot_projection_scores(neutral_profession_names, n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the direct bias measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep exploring the racial bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Open the [word embedding demo page in `responsibly` documentation](http://docs.responsibly.ai/notebooks/demo-word-embedding-bias.html#it-is-possible-also-to-expirements-with-new-target-word-sets-as-in-this-example-citizen-immigrant), and look on the use of the function [`calc_weat_pleasant_unpleasant_attribute`](). What was the attempt in that experiment? What was the result? Can you come up with other experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import calc_weat_pleasant_unpleasant_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Part Twelfth: Examples of Representation Bias in the Context of Gender\n",
    "\n",
    "![](images/examples-gender-bias-nlp.png)\n",
    "\n",
    "<small>Source: Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... & Wang, W. Y. (2019). [Mitigating Gender Bias in Natural Language Processing: Literature Review](https://arxiv.org/pdf/1906.08976.pdf). arXiv preprint arXiv:1906.08976.</small>\n",
    "\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "# üíéüíé Part Twelve: Takeaways\n",
    "\n",
    "1. **Downstream application** - putting a system into a human context\n",
    "\n",
    "2. **Measurements** (a.k.a \"what is a *good* system?\")\n",
    "\n",
    "3. **Data** (generation process, corpus building, selection bias, train vs. validation vs. test datasets)\n",
    "\n",
    "4. **Impact** of a system on individuals, groups, society, and humanity, both for **long-term** and **scale-up**\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "# Resources\n",
    "\n",
    "### [Doing Data Science Responsibly - Resources](https://handbook.responsibly.ai/appendices/resources.html)\n",
    "\n",
    "\n",
    "## Non-Technical Overview with More Downstream Application Examples\n",
    "- [Google - Text Embedding Models Contain Bias. Here's Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)\n",
    "- [Kai-Wei Chang (UCLA) - What It Takes to Control Societal Bias in Natural Language Processing](https://www.youtube.com/watch?v=RgcXD_1Cu18)\n",
    "- Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... & Wang, W. Y. (2019). [Mitigating Gender Bias in Natural Language Processing: Literature Review](https://arxiv.org/pdf/1906.08976.pdf). arXiv preprint arXiv:1906.08976.\n",
    "\n",
    "## Additional Related Work\n",
    "\n",
    "- **Understanding Bias**\n",
    "    - Ethayarajh, K., Duvenaud, D., & Hirst, G. (2019, July). [Understanding Undesirable Word Embedding Associations](https://arxiv.org/pdf/1908.06361.pdf). In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1696-1705). - **Including critical analysis of the current metrics and debiasing methods (quite technical)**\n",
    "\n",
    "  - Brunet, M. E., Alkalay-Houlihan, C., Anderson, A., & Zemel, R. (2019, May). [Understanding the Origins of Bias in Word Embeddings](https://arxiv.org/pdf/1810.03611.pdf). In International Conference on Machine Learning (pp. 803-811).\n",
    "\n",
    "\n",
    "- **Discovering Biases**\n",
    "  - Swinger, N., De-Arteaga, M., Heffernan IV, N. T., Leiserson, M. D., & Kalai, A. T. (2019, January). [What are the biases in my word embedding?](https://arxiv.org/pdf/1812.08769.pdf). In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 305-311). ACM.\n",
    "    Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories\n",
    "  \n",
    "  - Chaloner, K., & Maldonado, A. (2019, August). [Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories](https://www.aclweb.org/anthology/W19-3804). In Proceedings of the First Workshop on Gender Bias in Natural Language Processing (pp. 25-32).\n",
    "\n",
    "\n",
    "- **Fairness in Classification**\n",
    "  - Prost, F., Thain, N., & Bolukbasi, T. (2019, August). [Debiasing Embeddings for Reduced Gender Bias in Text Classification](https://arxiv.org/pdf/1908.02810.pdf). In Proceedings of the First Workshop on Gender Bias in Natural Language Processing (pp. 69-75).\n",
    "  \n",
    "  - Romanov, A., De-Arteaga, M., Wallach, H., Chayes, J., Borgs, C., Chouldechova, A., ... & Kalai, A. (2019, June). [What's in a Name? Reducing Bias in Bios without Access to Protected Attributes](https://arxiv.org/pdf/1904.05233.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4187-4195).\n",
    "\n",
    "\n",
    "- **Other**\n",
    "  \n",
    "  - Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., & Chang, K. W. (2019, June). [Gender Bias in Contextualized Word Embeddings](https://arxiv.org/pdf/1904.03310.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 629-634). [slides](https://jyzhao.net/files/naacl19.pdf)\n",
    "\n",
    "  - Zhou, P., Shi, W., Zhao, J., Huang, K. H., Chen, M., & Chang, K. W. [Analyzing and Mitigating Gender Bias in Languages with Grammatical Gender and Bilingual Word Embeddings](https://aiforsocialgood.github.io/icml2019/accepted/track1/pdfs/47_aisg_icml2019.pdf). ICML 2019 - AI for Social Good. [Poster](https://aiforsocialgood.github.io/icml2019/accepted/track1/posters/47_aisg_icml2019.pdf)\n",
    "\n",
    "\n",
    "##### Complete example of using `responsibly` with Word2Vec, GloVe and fastText: http://docs.responsibly.ai/notebooks/demo-gender-bias-words-embedding.html\n",
    "\n",
    "\n",
    "## Bias in NLP\n",
    "\n",
    "Around dozen of papers on this field until 2019, but nowdays plenty of work is done.\n",
    "- [1st ACL Workshop on Gender Bias for Natural Language Processing](https://genderbiasnlp.talp.cat/)\n",
    "- [NAACL 2019](https://naacl2019.org/)\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "<center><h1>THE END!</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
